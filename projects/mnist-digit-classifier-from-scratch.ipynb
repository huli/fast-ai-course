{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Digit Classifier with Fastai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision.all import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#2) [Path('training'),Path('testing')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# downloading and unpacking the files\n",
    "path = untar_data(URLs.MNIST)\n",
    "\n",
    "# setting the working path\n",
    "Path.BASE_PATH = path\n",
    "\n",
    "# inspect our directory\n",
    "path.ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((#10) [Path('training/9'),Path('training/0'),Path('training/7'),Path('training/6'),Path('training/1'),Path('training/8'),Path('training/4'),Path('training/3'),Path('training/2'),Path('training/5')],\n",
       " (#5842) [Path('training/4/27980.png'),Path('training/4/30542.png'),Path('training/4/29007.png'),Path('training/4/11598.png'),Path('training/4/26334.png'),Path('training/4/37239.png'),Path('training/4/32341.png'),Path('training/4/17857.png'),Path('training/4/29761.png'),Path('training/4/36641.png')...])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# verify if we have all numbers\n",
    "(path/'training').ls(), (path/'training'/'4').ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building a dictionary with all images\n",
    "def load_images(folder):\n",
    "    images = dict()\n",
    "    for digit in range(10):\n",
    "        images[digit] = [tensor(Image.open(f)) for f in (path/folder/str(digit)).ls()]\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load training and testing set\n",
    "all_images = load_images('training')\n",
    "testing_images = load_images('testing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split all images in training and validation set\n",
    "training_images = dict()\n",
    "validation_images = dict()\n",
    "for digit in range(10):\n",
    "    idx = math.floor(len(all_images[digit]) * .8)\n",
    "    training_images[digit] = all_images[digit][:idx]\n",
    "    validation_images[digit] = all_images[digit][idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEQAAABECAYAAAA4E5OyAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/d3fzzAAAACXBIWXMAAAsTAAALEwEAmpwYAAAEYklEQVR4nO2cPW8cRRiAn5ndvT3fh89fsR3HTowTEjAyEgQSihRICQgpBRU1DRU/ghbokdLQg0QRQCIUgEAhQYpS4AIUooSYO8eRzz5/BJ/vbr9mKBwSvHLcZWZF9iln7/S+evbdeWfnRie01uQ8QtpOIGvkQlLkQlLkQlLkQlK4+118Q77zv21B36kvxF7jeYWkyIWkyIWkyIWk2HdSNYksFhF9feipMYLxCm47wulEiLtNktaasTwyI0RMHqQ3PUT9XcVnZy7wyfJZrjWmGf38GKWL5oRk5pERUYzbiXELCad8j6OlFpVSD2X4lmVGSLx4D3n9BuF6kUgntBOfblBAJmbzyIyQNNuJT9D1kJHZtWFm5hDZV0RWK+ApABrbg4gVH7cTmc3DaLR9kKMjhM9O4PcHAMxUWsiJLlHVMZuH0Wj7sD07SuOtIi8fugvAeyM/8+HJi2xNmi3izAjRUqAccB/Moh6KsgzQhjPMjJCskBkhvUEHfbjL0VLLah6Z6TJbU4L3X7zMmfJNbN6nzAhRBZjxV6iKCPC53jvM9xuzFO4/peuQpKiZKywzJHeq48uVl5ifn2F6KTSah3Uh7vgYanyYeDiiJMATO0K2Ix+nI5HxU1Yh0cw4K6+UOTS5TE0W8MTOQqwdFfDaAhmafZmx3mW0I0l88N0YR6T2fS1scVsXojxJUoQ+1+w7y+OwLoQ9fwywh30hGcP6pBoMuHQnI6bL60gk8r8lY6F6rAvpDUgmjqxyorQMgPOg7dp6kuwJkQ7CcwkGBWfH7nDCvwdAoCMSrWmu9zNU17ibXUw2XmtChOMgfZ+wpjlfm2fKaaPw6emYQCuSDZ/anS5i42+jeVkT4kxN0Dl+gHAyZNptU33wqFzpDXJ16zjluoPXaKH+3jKalzUh0ViNtVmP8fEmY47/cPzXzjTfNmapLCrixbvG88pc2/16cQ714xD9f25biZ85IatLAxy8uoXbWLES35oQVXQIBzQ1v7dr3KuGtI+U0LWqlbysCUl8h6ii6S/sFlIuBXRGJKriP+abTxZrQtoTLi+c/ItzwzdspbAn1rpMWBO8PTrPXHFx7w8IAdIBrcDgaWvrS/c0SgtcDc1T/fDaaYZ/6+HfbqLWN1CdzhOPnzkhjlQoT9Ce1MSjEd6Wz8hqBbHdgadRyAfPf8NPh56j5napOD0+XXqT/noZf7MPNjaeePzMCTlfus/50jVaSZd15XChdo6k6IA0M/9nTsi/fLz6OpduzTJ6TVCab6A27xuJmzkhgY7o6YTrq4eRNytU6x3i5aax+JkT8lHrVb5amMP9YYBnrmzunEI0GN+aEBnCQnCAoowoiqWH45ebxwj+qDFyK0T/fhsVm92NtyZk4tISv9w6zRVXoLxHG4b+Wsix9TVYWSOJI6OLMrAoJF6o4y3U97xm+ODhLjL3+m+bXEiKXEiKXEiKXEiKXEiKXEiKXEgKkf8Zwm7yCkmRC0mRC0mRC0mRC0mRC0nxD2ebYMV4qvNUAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 72x72 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 72x72 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 72x72 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# lets inspect to samples\n",
    "for sample in (training_images[1][3], validation_images[4][2], testing_images[6][2]):\n",
    "    show_image(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create x and y tensors for all the sets\n",
    "train_x, train_y, valid_x, valid_y, test_x, test_y = \\\n",
    "    tensor([]), tensor([]), tensor([]), tensor([]), tensor([]), tensor([])\n",
    "for digit in range(10):\n",
    "    # training set\n",
    "    train_x = torch.cat([train_x, torch.stack(training_images[digit])])\n",
    "    train_y = torch.cat([train_y, tensor([digit]*len(training_images[digit])).unsqueeze(1)])\n",
    "    # validation set\n",
    "    valid_x = torch.cat([valid_x, torch.stack(validation_images[digit])])\n",
    "    valid_y = torch.cat([valid_y, tensor([digit]*len(validation_images[digit])).unsqueeze(1)])\n",
    "    # testing set\n",
    "    test_x = torch.cat([test_x, torch.stack(testing_images[digit])])\n",
    "    test_y = torch.cat([test_y, tensor([digit]*len(testing_images[digit])).unsqueeze(1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([47995, 28, 28]),\n",
       " torch.Size([47995, 1]),\n",
       " torch.Size([12005, 28, 28]),\n",
       " torch.Size([12005, 1]),\n",
       " torch.Size([10000, 28, 28]),\n",
       " torch.Size([10000, 1]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape, train_y.shape, valid_x.shape, valid_y.shape, test_x.shape, test_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([47995, 784]), torch.Size([12005, 784]), torch.Size([10000, 784]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# because we don't want to keep any spatial information we just \n",
    "# reshape the matrix of images to a vector\n",
    "train_x = train_x.view((-1, 28*28))\n",
    "valid_x = valid_x.view((-1, 28*28))\n",
    "test_x = test_x.view((-1, 28*28))\n",
    "train_x.shape, valid_x.shape, test_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([47995, 1]), torch.Size([12005, 1]), torch.Size([10000, 1]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# and our targes\n",
    "train_y.shape, valid_y.shape, test_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.],\n",
       "         [0.],\n",
       "         [0.]]),\n",
       " 2)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for pytorch we need 1d target tensors\n",
    "train_y[:3], train_y.ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so we squeeze the dimentions\n",
    "train_y = train_y.squeeze().long()\n",
    "valid_y = valid_y.squeeze().long()\n",
    "test_y = test_y.squeeze().long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0, 0, 0]), 1)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y[:3], train_y.ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([47995, 784]), torch.Size([12005, 784]), torch.Size([10000, 784]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape, valid_x.shape, test_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([47995]), torch.Size([12005]), torch.Size([10000]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y.shape, valid_y.shape, test_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data set and dataloaders \n",
    "ds_train = list(zip(train_x, train_y))\n",
    "ds_valid = list(zip(valid_x, valid_y))\n",
    "\n",
    "dl_train = DataLoader(ds_train, batch_size=256, shuffle=False)\n",
    "dl_valid = DataLoader(ds_valid, batch_size=256, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a Simple Dense Connected Network (without Learner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate network\n",
    "simple_net = nn.Sequential(\n",
    "    nn.Linear(28*28, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, 32),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(32, 10)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the optimizer\n",
    "class BasicOptimizer:\n",
    "    def __init__(self,params,lr): \n",
    "        self.params,self.lr = list(params),lr\n",
    "\n",
    "    def step(self, *args, **kwargs):\n",
    "        for p in self.params: \n",
    "            p.data -= p.grad.data * self.lr\n",
    "\n",
    "    def zero_grad(self, *args, **kwargs):\n",
    "        for p in self.params: \n",
    "            p.grad = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 784])\n",
      "torch.Size([64])\n",
      "torch.Size([32, 64])\n",
      "torch.Size([32])\n",
      "torch.Size([10, 32])\n",
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "# inspect weights and biases\n",
    "param = simple_net.parameters()\n",
    "for p in param:\n",
    "    print(p.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate gradient descent optimizer\n",
    "optimizer = BasicOptimizer(simple_net.parameters(), 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function for validating loss\n",
    "def validate_epoch(model):\n",
    "    predictions = [p.argmax().item() \n",
    "                   for xb, _ in dl_valid \n",
    "                       for p in simple_net.forward(xb)]\n",
    "    truths = [xy for xb, xy in dl_valid]\n",
    "    truths = torch.cat(truths).numpy()\n",
    "    return round((np.array(predictions)==np.array(truths)).mean(), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define method for a complete epoch\n",
    "def train_epoch(model):\n",
    "    for x, y in dl_train:\n",
    "        y_hat = model.forward(x)\n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "        loss.backward()\n",
    "        for p in model.parameters():\n",
    "            p.data -= p.grad*1e-5\n",
    "            p.grad.zero_()\n",
    "        #optimizer.step()\n",
    "        #optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0933"
      ]
     },
     "execution_count": 403,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# current loss\n",
    "validate_epoch(simple_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1764"
      ]
     },
     "execution_count": 404,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets train for one epoch and see how we do\n",
    "train_epoch(simple_net)\n",
    "validate_epoch(simple_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epochs(n=10):\n",
    "    for e in range(n):\n",
    "        train_epoch(simple_net)\n",
    "        print(validate_epoch(simple_net), end= ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2555 0.3193 0.3678 0.4034 0.4327 0.4596 0.483 0.5055 0.5265 0.5459 0.5612 0.5761 0.5888 0.6044 0.6173 0.6277 0.6362 0.6453 0.6544 0.6625 0.67 0.6775 0.6844 0.6912 0.6981 0.7034 0.7081 0.7126 0.7186 0.7227 0.7263 0.7317 0.7363 0.7389 0.7433 0.7474 0.7512 0.7547 0.7581 0.761 0.7633 0.7658 0.7696 0.7727 0.7753 0.7776 0.7797 0.7817 0.7836 0.7853 "
     ]
    }
   ],
   "source": [
    "train_epochs(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That seems to work but only with not shuffling (DataLoader(ds_train, batch_size=256, shuffle=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WHY THE HELL DOES IT NOT WORK WITH OPTIMIZER - WHAT ABOUT SHUFFLE!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0)"
      ]
     },
     "execution_count": 407,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 72x72 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# checking on a random sample\n",
    "show_image(test_x[1200].view((28, 28))); test_y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2)"
      ]
     },
     "execution_count": 408,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = simple_net.forward(test_x[1200])\n",
    "result.argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating validation on the test set by hand\n",
    "corrects = np.array([])\n",
    "for i, target in zip(test_x, test_y):\n",
    "    pred_label = simple_net.forward(i).argmax()\n",
    "    corrects = np.append(corrects, pred_label==target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10000, 784]), (10000,))"
      ]
     },
     "execution_count": 410,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_x.shape, corrects.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the testset: 79.63%\n"
     ]
    }
   ],
   "source": [
    "# check result\n",
    "print(f'Accuracy on the testset: {corrects.mean()*100}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a Locally Connected Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating loader\n",
    "image_data_loader = ImageDataLoaders.from_folder(path, train='training', valid='testing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiating learner with resnet architecture\n",
    "learn = cnn_learner(image_data_loader, resnet18, pretrained=False,\n",
    "                    loss_func=F.cross_entropy, metrics=accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.084031</td>\n",
       "      <td>0.649272</td>\n",
       "      <td>0.986800</td>\n",
       "      <td>13:10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# fine tuning for one cycle\n",
    "learn.fit_one_cycle(1, 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected locally connected layers yield much better results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
